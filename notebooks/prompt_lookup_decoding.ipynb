{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ff33a1386c841efb97525007f6a0067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_531c6a6be62f4141b3a72c6f8ac135a4",
              "IPY_MODEL_a6de573f4fa74c048bc77c3eb1c0ffd2",
              "IPY_MODEL_21170187d1fe472fb153899dcdc6e5b9"
            ],
            "layout": "IPY_MODEL_aa0e9944244b49899ca1b2b37407b33b"
          }
        },
        "531c6a6be62f4141b3a72c6f8ac135a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb700e27aadc4a84912cb17a192ddb97",
            "placeholder": "​",
            "style": "IPY_MODEL_1b2af66392894462a89cba792baa805b",
            "value": "generation_config.json: 100%"
          }
        },
        "a6de573f4fa74c048bc77c3eb1c0ffd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_028f65a75ade43f58862c53f386fd451",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ba058a19eb4740b99e602e36afa584",
            "value": 120
          }
        },
        "21170187d1fe472fb153899dcdc6e5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a63975d32ce64ac8b59828986240f6a7",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c0993c06a84a94a6cd2b807a198582",
            "value": " 120/120 [00:00&lt;00:00, 7.15kB/s]"
          }
        },
        "aa0e9944244b49899ca1b2b37407b33b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb700e27aadc4a84912cb17a192ddb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b2af66392894462a89cba792baa805b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "028f65a75ade43f58862c53f386fd451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ba058a19eb4740b99e602e36afa584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a63975d32ce64ac8b59828986240f6a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c0993c06a84a94a6cd2b807a198582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq6qNsn_HZxJ",
        "outputId": "9d872649-73b0-4d53-8de5-db0219f61798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.1.0+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (2.15.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (2.1.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.5.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.24.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.6)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.0)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.35.2)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.8.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers optimum\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "TgJSIon9HlXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"cuda:0\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             revision=\"main\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569,
          "referenced_widgets": [
            "5ff33a1386c841efb97525007f6a0067",
            "531c6a6be62f4141b3a72c6f8ac135a4",
            "a6de573f4fa74c048bc77c3eb1c0ffd2",
            "21170187d1fe472fb153899dcdc6e5b9",
            "aa0e9944244b49899ca1b2b37407b33b",
            "cb700e27aadc4a84912cb17a192ddb97",
            "1b2af66392894462a89cba792baa805b",
            "028f65a75ade43f58862c53f386fd451",
            "52ba058a19eb4740b99e602e36afa584",
            "a63975d32ce64ac8b59828986240f6a7",
            "e1c0993c06a84a94a6cd2b807a198582"
          ]
        },
        "id": "UQUJ5tajHo5B",
        "outputId": "0e01399c-6343-48d9-a122-5679cfe26a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ff33a1386c841efb97525007f6a0067"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32002, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "          (k_proj): QuantLinear()\n",
              "          (o_proj): QuantLinear()\n",
              "          (q_proj): QuantLinear()\n",
              "          (v_proj): QuantLinear()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (act_fn): SiLUActivation()\n",
              "          (down_proj): QuantLinear()\n",
              "          (gate_proj): QuantLinear()\n",
              "          (up_proj): QuantLinear()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm()\n",
              "        (post_attention_layernorm): MistralRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import inspect\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import nn\n",
        "\n",
        "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
        "from transformers.models.auto import (\n",
        "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
        "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
        "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
        ")\n",
        "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
        "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
        "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
        "from transformers.generation.configuration_utils import GenerationConfig\n",
        "from transformers.generation.logits_process import (\n",
        "    EncoderNoRepeatNGramLogitsProcessor,\n",
        "    EncoderRepetitionPenaltyLogitsProcessor,\n",
        "    EpsilonLogitsWarper,\n",
        "    EtaLogitsWarper,\n",
        "    ExponentialDecayLengthPenalty,\n",
        "    ForcedBOSTokenLogitsProcessor,\n",
        "    ForcedEOSTokenLogitsProcessor,\n",
        "    ForceTokensLogitsProcessor,\n",
        "    HammingDiversityLogitsProcessor,\n",
        "    InfNanRemoveLogitsProcessor,\n",
        "    LogitNormalization,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    MinNewTokensLengthLogitsProcessor,\n",
        "    NoBadWordsLogitsProcessor,\n",
        "    NoRepeatNGramLogitsProcessor,\n",
        "    PrefixConstrainedLogitsProcessor,\n",
        "    RepetitionPenaltyLogitsProcessor,\n",
        "    SequenceBiasLogitsProcessor,\n",
        "    SuppressTokensAtBeginLogitsProcessor,\n",
        "    SuppressTokensLogitsProcessor,\n",
        "    TemperatureLogitsWarper,\n",
        "    TopKLogitsWarper,\n",
        "    TopPLogitsWarper,\n",
        "    TypicalLogitsWarper,\n",
        "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
        ")\n",
        "from transformers.generation.stopping_criteria import (\n",
        "    MaxLengthCriteria,\n",
        "    MaxTimeCriteria,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    validate_stopping_criteria,\n",
        ")\n",
        "\n",
        "from transformers.generation.utils import _crop_past_key_values\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for outputs of decoder-only generation models using greedy search.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
        "            if all batches finished early due to the `eos_token_id`.\n",
        "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
        "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
        "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
        "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
        "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
        "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
        "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
        "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
        "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
        "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
        "    \"\"\"\n",
        "\n",
        "    sequences: torch.LongTensor = None\n",
        "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "\n",
        "\n",
        "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
        "UNDERLINE = \"\\x1b[4m\"\n",
        "RESET = \"\\x1b[0m\"\n"
      ],
      "metadata": {
        "id": "5_--oQtAL3jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the \"draft model\" function (main thing to read)\n",
        "@torch.no_grad()\n",
        "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
        "    input_length = input_ids.size(1)\n",
        "\n",
        "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
        "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
        "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
        "\n",
        "    for ngram_size in range(max_ngram_size, 0, -1):\n",
        "        # Extract the last n tokens as our search ngram\n",
        "        ngram = input_ids[0, -ngram_size:].tolist()\n",
        "\n",
        "        # Create sliding windows of size ngram_size\n",
        "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
        "\n",
        "        # Convert ngram to a tensor for comparison\n",
        "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        # Find where the windows match the ngram\n",
        "        matches = (windows == ngram_tensor).all(dim=2)\n",
        "\n",
        "        # Get the indices of matches\n",
        "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Iterate through match indices to find a valid continuation\n",
        "        for idx in match_indices:\n",
        "            start_idx = idx + ngram_size\n",
        "            end_idx = start_idx + num_pred_tokens\n",
        "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
        "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
        "                return input_ids[0, start_idx:end_idx]\n",
        "\n",
        "    # If no match is found, return an empty tensor\n",
        "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "\n",
        "# prompt lookup decoding\n",
        "@torch.no_grad()\n",
        "def greedy_search_pld(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "        max_length: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_scores: Optional[bool] = None,\n",
        "        return_dict_in_generate: Optional[bool] = None,\n",
        "        synced_gpus: bool = False,\n",
        "        streamer: Optional[\"BaseStreamer\"] = None,\n",
        "        draft_matching_window_size = 3,\n",
        "        draft_num_candidate_tokens = 10,\n",
        "        print_output=True,\n",
        "        **model_kwargs,\n",
        "    ):\n",
        "        global tokenizer\n",
        "\n",
        "        # init values\n",
        "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
        "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
        "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
        "\n",
        "        # # init attention / hidden states / scores tuples\n",
        "        scores = () if (return_dict_in_generate and output_scores) else None\n",
        "\n",
        "        max_len = stopping_criteria[0].max_length\n",
        "\n",
        "        i = 0\n",
        "        current_color_index = 0\n",
        "\n",
        "        dummy_candidate_input_ids = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        while True:\n",
        "            i += 1\n",
        "            cur_len = input_ids.shape[-1]\n",
        "\n",
        "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
        "\n",
        "            if len(candidate_pred_tokens) == 0:\n",
        "                candidate_pred_tokens = dummy_candidate_input_ids # todo: fix code so that this isn't needed\n",
        "            else:\n",
        "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
        "\n",
        "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
        "\n",
        "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
        "\n",
        "            candidate_kwargs = copy.copy(model_kwargs)\n",
        "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
        "\n",
        "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
        "\n",
        "\n",
        "            # forward pass to get next token(s)\n",
        "            outputs = self(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "            )\n",
        "\n",
        "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
        "            selected_tokens = new_logits.argmax(dim=-1)\n",
        "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
        "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
        "\n",
        "\n",
        "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
        "            #     n_matches -= 1\n",
        "\n",
        "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
        "\n",
        "            # print(n_matches)\n",
        "            # i+= n_matches.item()\n",
        "\n",
        "            if print_output:\n",
        "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
        "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
        "            new_cur_len = input_ids.shape[-1]\n",
        "\n",
        "            if print_output:\n",
        "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "                # Find and print the newly added text\n",
        "                if updated_text != current_text:\n",
        "                    new_text = updated_text[len(current_text):]\n",
        "                    if len(valid_tokens[0]) > 1:\n",
        "                        color = COLORS[current_color_index]\n",
        "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
        "                        # Update color for next generation\n",
        "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
        "                    else:\n",
        "                        print(f\"{new_text}\", end='')\n",
        "\n",
        "            new_cache_size = new_cur_len - 1\n",
        "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
        "\n",
        "\n",
        "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
        "\n",
        "            # stop if we exceed the maximum length\n",
        "\n",
        "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
        "                break\n",
        "\n",
        "            if stopping_criteria(input_ids, scores):\n",
        "                break\n",
        "\n",
        "\n",
        "        if return_dict_in_generate:\n",
        "            return GreedySearchDecoderOnlyOutput(\n",
        "                sequences=input_ids,\n",
        "                scores=scores,\n",
        "                # attentions=decoder_attentions,\n",
        "                # hidden_states=decoder_hidden_states,\n",
        "            )\n",
        "        else:\n",
        "            return input_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "D-LxOqC_Hr8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))"
      ],
      "metadata": {
        "id": "VNkfMtJeNNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_text = \"\"\"import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the average\n",
        "average_throughput = np.mean(tokens_per_sec_arr)\n",
        "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Histogram of Throughput Values')\n",
        "plt.xlabel('Tokens per Second')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
        "plt.show()\n",
        "\"\"\"\n",
        "question = \"Can you please change x axis to start from 0\"\n",
        "prompt = \"\"\"<|im_start|>system\n",
        "You are a helpful assistant<|im_end|>\n",
        "<|im_start|>user\n",
        "```python\\n{code_text}``` \\n\\n{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\".format(code_text=code_text, question=question)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Move all tensor values in the inputs to GPU\n",
        "for key in inputs:\n",
        "    inputs[key] = inputs[key].to(device)"
      ],
      "metadata": {
        "id": "hzc4BQAmNOux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
        "\n",
        "# Define the variable for max_new_tokens\n",
        "max_new_tokens = 500\n",
        "use_new_generate = True\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate the output\n",
        "\n",
        "if not use_new_generate:\n",
        "    # this doesn't stream unfortunately\n",
        "    out = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=0,\n",
        "                         do_sample=False,\n",
        "                         return_dict_in_generate=True)\n",
        "else:\n",
        "    out = model.greedy_search_pld(inputs.input_ids,\n",
        "                              attention_mask = inputs.attention_mask,\n",
        "                              stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
        "                              draft_matching_window_size = 3,\n",
        "                              draft_num_candidate_tokens = 10,\n",
        "                              use_cache=True,\n",
        "                              pad_token_id=0,\n",
        "                              return_dict_in_generate=True)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "out_text = tokenizer.batch_decode(out.sequences,)[0]\n",
        "\n",
        "if not use_new_generate:\n",
        "  print(out_text)\n",
        "num_tokens_generated = len(out.sequences[0]) - len(inputs['input_ids'][0])\n",
        "\n",
        "total_time = end_time - start_time\n",
        "tokens_per_sec = num_tokens_generated / total_time\n",
        "\n",
        "print(f\"\\n\\nTotal time: {total_time} seconds\")\n",
        "print(f\"Tokens per second: {tokens_per_sec} tokens/sec\")\n",
        "print(f\"Total tokens generated: {num_tokens_generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqo4eZqINQmG",
        "outputId": "c00fdd57-dd77-4685-86cc-775fb792a27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure, you can change the x\u001b[31m axis to start from 0 by\u001b[0m modifying the code as follows:\n",
            "\n",
            "``\u001b[32m`python\n",
            "import numpy as np\n",
            "import matplot\u001b[0m\u001b[34mlib.pyplot as plt\n",
            "\n",
            "# Calculate\u001b[0m\u001b[35m the average\n",
            "average_throughput = np.\u001b[0m\u001b[31mmean(tokens_per_sec_arr)\n",
            "\u001b[0m\u001b[32mprint(f\"Average Throughput: {a\u001b[0m\u001b[34mverage_throughput} tokens/sec\")\n",
            "\n",
            "\u001b[0m\u001b[35m# Plotting the histogram\n",
            "plt.hist\u001b[0m\u001b[31m(tokens_per_sec_arr, bins\u001b[0m\u001b[32m=20, color='blue', edgecolor='\u001b[0m\u001b[34mblack', alpha=0.7)\n",
            "plt\u001b[0m\u001b[35m.title\u001b[0m\u001b[31m('Histogram of Throughput Values')\n",
            "pl\u001b[0m\u001b[32mt.xlabel('Tokens per Second')\n",
            "pl\u001b[0m\u001b[34mt.y\u001b[0m\u001b[35mlabel('Frequency')\n",
            "plt.axv\u001b[0m\u001b[31mline(average_throughput, color='red\u001b[0m\u001b[32m', linestyle='dashed', linewidth=1\u001b[0m\u001b[34m)\n",
            "plt.text(average_through\u001b[0m\u001b[35mput*\u001b[0m\u001b[31m0.9, max(plt.ylim\u001b[0m\u001b[32m())*0.9, f'Average:\u001b[0m\u001b[34m {average_throughput:.2f}',\u001b[0m\u001b[35m color = 'red')\n",
            "plt.x\u001b[0mlim(0\u001b[31m, max\u001b[0m\u001b[32m(plt.x\u001b[0mlim()))\n",
            "pl\u001b[34mt.show\u001b[0m\u001b[35m()\n",
            "```\n",
            "\u001b[0m\n",
            "In this modified code, I added the line `pl\u001b[31mt.x\u001b[0mlim\u001b[32m(0, max(plt.xlim()))\u001b[0m` to set the x\u001b[34m axis limits\u001b[0m to\u001b[35m start from 0.\u001b[0m\n",
            "\n",
            "Total time: 10.873536109924316 seconds\n",
            "Tokens per second: 28.78548402615077 tokens/sec\n",
            "Total tokens generated: 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4QzvHXGOS3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}