{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJF5nIOOoItM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#install required libraries\n",
        "!pip install accelerate bitsandbytes tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, GenerationConfig, TextStreamer\n",
        "\n",
        "from typing import Union, Tuple, List"
      ],
      "metadata": {
        "id": "qUncBGLHFFU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def instantiate_huggingface_model(\n",
        "    model_name,\n",
        "    quantization_config=None,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=True,\n",
        "    trust_remote_code=None,\n",
        "    pad_token=None,\n",
        "    padding_side=\"left\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Instantiate a HuggingFace model with optional quantization using the BitsAndBytes library.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): The name of the model to load from HuggingFace's model hub.\n",
        "    - quantization_config (BitsAndBytesConfig, optional): Configuration for model quantization.\n",
        "      If None, defaults to a pre-defined quantization configuration for 4-bit quantization.\n",
        "    - device_map (str, optional): Device placement strategy for model layers ('auto' by default).\n",
        "    - use_cache (bool, optional): Whether to cache model outputs (False by default).\n",
        "    - trust_remote_code (bool, optional): Whether to trust remote code for custom layers (True by default).\n",
        "    - pad_token (str, optional): The pad token to be used by the tokenizer. If None, uses the EOS token.\n",
        "    - padding_side (str, optional): The side on which to pad the sequences ('left' by default).\n",
        "\n",
        "    Returns:\n",
        "    - model (PreTrainedModel): The instantiated model ready for inference or fine-tuning.\n",
        "    - tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
        "\n",
        "    The function will throw an exception if model loading fails.\n",
        "    \"\"\"\n",
        "\n",
        "    # If quantization_config is not provided, use the default configuration\n",
        "    if quantization_config is None:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=device_map,\n",
        "        use_cache=use_cache,\n",
        "        trust_remote_code=trust_remote_code\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                              trust_remote_code=trust_remote_code)\n",
        "\n",
        "    if pad_token is not None:\n",
        "        tokenizer.pad_token = pad_token\n",
        "    else:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    tokenizer.padding_side = padding_side\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "t7pfuq2aTaAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = instantiate_huggingface_model(\"Deci/DeciCoder-6B\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "qQvrN5XUT6gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "deci_generator = pipeline(\"text-generation\",\n",
        "                          model=model,\n",
        "                          tokenizer=tokenizer,\n",
        "                          device_map=\"auto\",\n",
        "                          max_length=2048,\n",
        "                          temperature=1e-3,\n",
        "                          do_sample=True,\n",
        "                          streamer=streamer\n",
        ")"
      ],
      "metadata": {
        "id": "cfVCsbzHVYBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(pipeline, prompt):\n",
        "    \"\"\"\n",
        "    Generates text using a specified pipeline based on a given prompt.\n",
        "\n",
        "    This function takes a pipeline object, which is typically a pre-trained\n",
        "    model loaded via the transformers library, and a prompt string. It then\n",
        "    uses the pipeline to generate text based on the prompt.\n",
        "\n",
        "    :param pipeline: A pre-initialized pipeline object for text generation.\n",
        "    :param prompt: A string containing the prompt to guide text generation.\n",
        "    :return: A string containing the generated text.\n",
        "    \"\"\"\n",
        "    result = pipeline(prompt)[0]['generated_text']"
      ],
      "metadata": {
        "id": "PxHtvC31xNQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt"
      ],
      "metadata": {
        "id": "P_SDDp9QflGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def all_odd_elements(sequence):\\n    \\\"\\\"\\\"Returns every odd element of the sequence.\\\"\\\"\\\"\"\n",
        "generate_text(deci_generator, prompt)"
      ],
      "metadata": {
        "id": "AhNOWS-vQA_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f5f955-2ef4-480a-84c5-0a70f63009e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def all_odd_elements(sequence):\n",
            "    \"\"\"Returns every odd element of the sequence.\"\"\"\n",
            "    return [x for x in sequence if x % 2 != 0]<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def nth_element_in_fibonnaci(element):\\n    \\\"\\\"\\\"Returns the nth element of the Fibonnaci sequence.\\\"\\\"\\\"\"\n",
        "generate_text(deci_generator, prompt)"
      ],
      "metadata": {
        "id": "F3Crs-CiH_F2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a59b6a-16a9-4eec-fd39-c63655922004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def nth_element_in_fibonnaci(element):\n",
            "    \"\"\"Returns the nth element of the Fibonnaci sequence.\"\"\"\n",
            "    if element == 0:\n",
            "        return 0\n",
            "    elif element == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return nth_element_in_fibonnaci(element - 1) + nth_element_in_fibonnaci(element - 2)\n",
            "\n",
            "def fibonacci_sequence(n):\n",
            "    \"\"\"Returns the Fibonacci sequence up to n elements.\"\"\"\n",
            "    sequence = []\n",
            "    for i in range(n):\n",
            "        sequence.append(nth_element_in_fibonnaci(i))\n",
            "    return sequence\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    n = int(input('Enter the number of elements: '))\n",
            "    print(fibonacci_sequence(n))<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct"
      ],
      "metadata": {
        "id": "OjFogRcGp6tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write code in Python to check if a number is prime\"\n",
        "generate_text(deci_generator, prompt)"
      ],
      "metadata": {
        "id": "-KeJwRyTqBVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86df275-241f-438d-d783-13ef4352746f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write code in Python to check if a number is prime or not.\n",
            "\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, n):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "# Test\n",
            "print(is_prime(7)) # True\n",
            "print(is_prime(8)) # False<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write code in Java to check if a number is prime\"\n",
        "generate_text(deci_generator, prompt)"
      ],
      "metadata": {
        "id": "odmX41Geqa3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78719aca-c3f5-479e-bad4-cb3c9a625a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write code in Java to check if a number is prime or not.\n",
            "\n",
            "public class PrimeNumber {\n",
            "    public static void main(String[] args) {\n",
            "        int n = 10;\n",
            "        boolean isPrime = true;\n",
            "        for (int i = 2; i < n; i++) {\n",
            "            if (n % i == 0) {\n",
            "                isPrime = false;\n",
            "                break;\n",
            "            }\n",
            "        }\n",
            "        if (isPrime) {\n",
            "            System.out.println(n + \" is a prime number\");\n",
            "        } else {\n",
            "            System.out.println(n + \" is not a prime number\");\n",
            "        }\n",
            "    }\n",
            "}<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write code in Rust to check if a number is prime\"\n",
        "generate_text(deci_generator, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOvckF13b-2x",
        "outputId": "b2841b14-5787-45b1-be48-ae344157ae3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write code in Rust to check if a number is prime or not.\n",
            "\n",
            "fn is_prime(n: u32) -> bool {\n",
            "    if n == 1 {\n",
            "        return false;\n",
            "    }\n",
            "    for i in 2..n {\n",
            "        if n % i == 0 {\n",
            "            return false;\n",
            "        }\n",
            "    }\n",
            "    true\n",
            "}\n",
            "\n",
            "fn main() {\n",
            "    let n = 5;\n",
            "    println!(\"Is {} prime? {}\", n, is_prime(n));\n",
            "}<|endoftext|>\n"
          ]
        }
      ]
    }
  ]
}