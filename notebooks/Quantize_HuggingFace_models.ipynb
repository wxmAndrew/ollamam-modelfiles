{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiHa-Kim/quantize-hf-models/blob/main/Quantize_HuggingFace_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17G-9ywMsiP"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "MODEL_ID = \"macadeliccc/WestLake-7B-v2-laser-truthy-dpo\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
        "SOTA_QUANTIZATION_METHODS = [\"iq2_xxs\", \"iq2_xs\", \"iq3_xxs\"]\n",
        "\n",
        "# Dataset URL for the imatrix computation\n",
        "DATASET_NAME = \"wikitext/wikitext-2-raw-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO6y5ULwNV8L",
        "outputId": "88d46dbc-8cd2-44bd-b8b9-ca6849921c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3109, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 444, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 575, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 106, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 221, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2822, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3111, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3133, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3133, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3125, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 300, in evaluate\n",
            "    current_environment = default_environment()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 270, in default_environment\n",
            "    \"python_version\": \".\".join(platform.python_version_tuple()[:2]),\n",
            "  File \"/usr/lib/python3.10/platform.py\", line 1127, in python_version_tuple\n",
            "    return tuple(_sys_version()[1].split('.'))\n",
            "  File \"/usr/lib/python3.10/platform.py\", line 1016, in _sys_version\n",
            "    result = _sys_version_cache.get(sys_version, None)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 234, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 217, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy~=1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)\n",
            "Requirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.35.2)\n",
            "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.2)\n",
            "Requirement already satisfied: torch~=2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install -U pip\n",
        "!pip install -U huggingface_hub\n",
        "!pip install -U datasets\n",
        "!pip install -r /content/llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed2RCi25QULT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd /content/llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3NdmDP59myy4",
        "outputId": "9269b2b3-d551-47b6-f64c-e92f21148ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llama.cpp\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/llama.cpp'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd /content/llama.cpp\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwkDOlaMM6Y6"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "ORIG_PATH = f\"{MODEL_NAME}/orig\"\n",
        "QUANT_PATH = f\"{MODEL_NAME}/quant\"\n",
        "MODEL_FP16_PATH = f\"{QUANT_PATH}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "IMATRIX_PATH = f\"imatrix_{MODEL_NAME.lower()}.dat\"\n",
        "QUESTIONS_PATH = f\"{MODEL_NAME.lower()}/questions.txt\"\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import argparse\n",
        "from huggingface_hub import snapshot_download\n",
        "from datasets import load_dataset\n",
        "import pandas\n",
        "import torch\n",
        "\n",
        "N_GPU_LAYERS = 35\n",
        "HAS_GPU = torch.cuda.is_available()\n",
        "USE_GPU = f\"-ngl {N_GPU_LAYERS}\"\n",
        "\n",
        "# Functions\n",
        "def download_model(model_id: str=MODEL_ID):\n",
        "  \"\"\"Downloads a model from HuggingFace Hub.\n",
        "\n",
        "  Args:\n",
        "    model_id: The ID of the model to download.\n",
        "  \"\"\"\n",
        "  print(f\"Downloading {model_id}\")\n",
        "  os.makedirs(MODEL_NAME, exist_ok=True)\n",
        "  os.makedirs(ORIG_PATH, exist_ok=True)\n",
        "  os.makedirs(QUANT_PATH, exist_ok=True)\n",
        "  snapshot_download(\n",
        "      repo_id=model_id,\n",
        "      local_dir=ORIG_PATH,\n",
        "      local_dir_use_symlinks=False,\n",
        "      revision=\"main\",\n",
        "      ignore_patterns=\"*.bin\"\n",
        "  )\n",
        "  print(f\"Downloaded {model_id}\")\n",
        "  return\n",
        "\n",
        "def convert_model(output_path: str=MODEL_FP16_PATH):\n",
        "  \"\"\"Converts safetensors to FP16 GGUF.\n",
        "\n",
        "  Args:\n",
        "    outfile: The path to the output file.\n",
        "  \"\"\"\n",
        "  !python convert.py {ORIG_PATH} --outtype f16 --outfile {output_path}\n",
        "\n",
        "  print(f\"Outputted the FP16 GGUF file to {output_path}\")\n",
        "  return\n",
        "\n",
        "def quantize_model(methods: list, imatrix_path: str = None, fp16_path: str=MODEL_FP16_PATH, quant_path: str=QUANT_PATH) -> None:\n",
        "  \"\"\"Quantizes a model using different methods and saves the results in a given path.\n",
        "\n",
        "  Args:\n",
        "    fp16_path: The path of the model file to quantize.\n",
        "    quant_path: The path to save the quantized model files.\n",
        "    methods: The list of quantization methods to use.\n",
        "    imatrix_path: The path of the importance matrix file to use for SOTA methods. Default is None.\n",
        "  \"\"\"\n",
        "  print(f\"Quantizing {fp16_path} using {methods}\")\n",
        "  for method in methods:\n",
        "    model_name = fp16_path.split('/')[-1].split('.')[0]\n",
        "    qtype = f\"{quant_path}/{model_name.lower()}.{method.upper()}.gguf\"\n",
        "    if imatrix_path:\n",
        "      !/content/llama.cpp/quantize --imatrix {imatrix_path} {fp16_path} {qtype} {method}\n",
        "\n",
        "    elif not imatrix_path:\n",
        "      !/content/llama.cpp/quantize {fp16_path} {qtype} {method}\n",
        "\n",
        "def prepare_dataset(dataset_name: str=DATASET_NAME):\n",
        "  \"\"\"Load the dataset and write it to a file\n",
        "\n",
        "  Args:\n",
        "    dataset_name: Name of the dataset on HuggingFace,\n",
        "                  in the format tree/node e.g. \"wikitext/wikitext-2-raw-v1\"\n",
        "  \"\"\"\n",
        "  print(\"Writing dataset to 'wiki.train.raw'...\")\n",
        "  ds_tree, ds_root = DATASET_NAME.split('/')\n",
        "  dataset = load_dataset(ds_tree, ds_root)\n",
        "  # Convert the 'text' column of the training split to a raw text file\n",
        "  with open('wiki.train.raw', 'w') as file:\n",
        "    for article in dataset['train']['text']:\n",
        "      file.write(article + '\\n')\n",
        "  print(\"Wrote dataset to 'wiki.train.raw'\")\n",
        "\n",
        "def compute_imatrix(fp16_path: str=MODEL_FP16_PATH) -> str:\n",
        "  \"\"\"Computes the importance matrix for a model using a given dataset.\n",
        "\n",
        "  Args:\n",
        "    fp16_path: The path of the model file to use.\n",
        "    dataset: The path of the dataset file to use.\n",
        "\n",
        "  Returns:\n",
        "    The path of the importance matrix file.\n",
        "  \"\"\"\n",
        "\n",
        "  model_name = fp16_path.split('/')[-1].split('.')[0]\n",
        "  IMATRIX_PATH = f\"{model_name.lower()}.imatrix\"\n",
        "\n",
        "  if HAS_GPU:\n",
        "    !/content/llama.cpp/imatrix -m {fp16_path} -f wiki.train.raw -o {IMATRIX_PATH} --chunks 100 {USE_GPU}\n",
        "  else:\n",
        "    !/content/llama.cpp/imatrix -m {fp16_path} -f wiki.train.raw -o {IMATRIX_PATH} --chunks 100\n",
        "  return IMATRIX_PATH\n",
        "\n",
        "def generate_questions(prompt: str, quants: list= [\"IQ2_XSS\",\"Q4_K_M\"], quant_path: str=QUANT_PATH) -> None:\n",
        "  \"\"\"Generates questions using a quantized model and a given prompt.\n",
        "\n",
        "  Args:\n",
        "    quant_path: The path of the quantized model file to use.\n",
        "    prompt: The prompt to use for generating questions.\n",
        "  \"\"\"\n",
        "  model_name = quant_path.split('/')[0]\n",
        "  for quant in quants:\n",
        "    qtype = f\"{quant_path}/{model_name.lower()}.{quant}.gguf\"\n",
        "    print(f\"Generating questions using {qtype} and {prompt}...\")\n",
        "    !main -m {qtype} -n 128 -p {prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwj5WHANYc2"
      },
      "outputs": [],
      "source": [
        "# Download the model from HuggingFace\n",
        "download_model(MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrjsBk_2jAhM"
      },
      "outputs": [],
      "source": [
        "convert_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC8aG0iSNZPz"
      },
      "outputs": [],
      "source": [
        "# Quantize the model using classic methods\n",
        "quantize_model(QUANTIZATION_METHODS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNYvKH9hG7Nd"
      },
      "outputs": [],
      "source": [
        "prepare_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80_AOFTbNbG8"
      },
      "outputs": [],
      "source": [
        "# Compute the imatrix using the wikitext dataset\n",
        "IMATRIX_PATH = compute_imatrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iiAQLz6wC48"
      },
      "outputs": [],
      "source": [
        "# !cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C20r7WbHu7H",
        "outputId": "2e169342-843d-4021-8c81-225d227f4bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: /content/llama.cpp/quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights] [--exclude-weights] model-f32.gguf [model-quant.gguf] type [nthreads]\n",
            "\n",
            "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
            "  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n",
            "  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n",
            "  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\n",
            "  --include-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "Note: --include-weights and --exclude-weights cannot be used together\n",
            "\n",
            "Allowed quantization types:\n",
            "   2  or  Q4_0    :  3.56G, +0.2166 ppl @ LLaMA-v1-7B\n",
            "   3  or  Q4_1    :  3.90G, +0.1585 ppl @ LLaMA-v1-7B\n",
            "   8  or  Q5_0    :  4.33G, +0.0683 ppl @ LLaMA-v1-7B\n",
            "   9  or  Q5_1    :  4.70G, +0.0349 ppl @ LLaMA-v1-7B\n",
            "  19  or  IQ2_XXS :  2.06 bpw quantization\n",
            "  20  or  IQ2_XS  :  2.31 bpw quantization\n",
            "  10  or  Q2_K    :  2.63G, +0.6717 ppl @ LLaMA-v1-7B\n",
            "  21  or  Q2_K_S  :  2.16G, +9.0634 ppl @ LLaMA-v1-7B\n",
            "  23  or  IQ3_XXS :  3.06 bpw quantization\n",
            "  12  or  Q3_K    : alias for Q3_K_M\n",
            "  22  or  Q3_K_XS : 3-bit extra small quantization\n",
            "  11  or  Q3_K_S  :  2.75G, +0.5551 ppl @ LLaMA-v1-7B\n",
            "  12  or  Q3_K_M  :  3.07G, +0.2496 ppl @ LLaMA-v1-7B\n",
            "  13  or  Q3_K_L  :  3.35G, +0.1764 ppl @ LLaMA-v1-7B\n",
            "  15  or  Q4_K    : alias for Q4_K_M\n",
            "  14  or  Q4_K_S  :  3.59G, +0.0992 ppl @ LLaMA-v1-7B\n",
            "  15  or  Q4_K_M  :  3.80G, +0.0532 ppl @ LLaMA-v1-7B\n",
            "  17  or  Q5_K    : alias for Q5_K_M\n",
            "  16  or  Q5_K_S  :  4.33G, +0.0400 ppl @ LLaMA-v1-7B\n",
            "  17  or  Q5_K_M  :  4.45G, +0.0122 ppl @ LLaMA-v1-7B\n",
            "  18  or  Q6_K    :  5.15G, +0.0008 ppl @ LLaMA-v1-7B\n",
            "   7  or  Q8_0    :  6.70G, +0.0004 ppl @ LLaMA-v1-7B\n",
            "   1  or  F16     : 13.00G              @ 7B\n",
            "   0  or  F32     : 26.00G              @ 7B\n",
            "          COPY    : only copy tensors, no quantizing\n"
          ]
        }
      ],
      "source": [
        "# !/content/llama.cpp/quantize --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgzWsTi1NcMQ",
        "outputId": "2e2bd621-66d8-4e87-f7a4-f5342a952db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantizing WestLake-7B-v2-laser-truthy-dpo/quant/westlake-7b-v2-laser-truthy-dpo.fp16.bin using ['iq2_xxs', 'iq2_xs', 'iq3_xxs']\n",
            "load_imatrix: loaded 224 importance matrix entries from westlake-7b-v2-laser-truthy-dpo.imatrix\n",
            "prepare_imatrix: have 224 importance matrix entries\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 2061 (9392ebd4)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'WestLake-7B-v2-laser-truthy-dpo/quant/westlake-7b-v2-laser-truthy-dpo.fp16.bin' to 'WestLake-7B-v2-laser-truthy-dpo/quant/westlake-7b-v2-laser-truthy-dpo.IQ2_XXS.gguf' as IQ2_XXS\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from WestLake-7B-v2-laser-truthy-dpo/quant/westlake-7b-v2-laser-truthy-dpo.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = WestLake-7B-v2-laser-truthy-dpo\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "================================ Have weights data with 224 entries\n",
            "llama_model_quantize_internal: meta size = 735008 bytes\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, \n",
            "====== llama_model_quantize_internal: did not find weights for token_embd.weight\n",
            "quantizing to q2_K .. size =   250.00 MiB ->    41.02 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. ================================================================= iq2xs_init_impl(grid_size = 256)\n",
            "iq2xs_init_impl: 373964 neighbours in total\n",
            "size =   112.00 MiB ->    14.44 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  20/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  21/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  22/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  23/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  24/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  25/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  26/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  29/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  31/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  32/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  33/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  34/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  38/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  40/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  41/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  42/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  43/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  44/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  47/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  49/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  50/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  51/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  52/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  53/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  56/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  58/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  59/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  60/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  61/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  65/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  67/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  68/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  69/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  70/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  74/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  76/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  77/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  78/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  79/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  80/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  83/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  85/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  86/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  87/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  88/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  89/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  92/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  94/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[  95/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  96/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[  97/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  98/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 200/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
            "[ 201/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
            "[ 203/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, \n",
            "====== llama_model_quantize_internal: did not find weights for output.weight\n",
            "quantizing to q5_K .. size =   250.00 MiB ->    85.94 MiB\n",
            "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
            "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to iq2_xxs .. "
          ]
        }
      ],
      "source": [
        "# Quantize the model using imatrix methods\n",
        "IMATRIX_PATH = f\"{MODEL_NAME.lower()}.imatrix\"\n",
        "quantize_model(SOTA_QUANTIZATION_METHODS, IMATRIX_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCYcs6pUNdxW"
      },
      "outputs": [],
      "source": [
        "# Generate questions using the prompt\n",
        "PROMPT = \"User: Tell me story about what a quantization is and what we need to build.\"\n",
        "\n",
        "generate_questions(\n",
        "    QUANT_PATH,\n",
        "    PROMPT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS7gfA-bCAFV"
      },
      "outputs": [],
      "source": [
        "USER = \"Ji-Ha\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJZGt9AIBn7W"
      },
      "outputs": [],
      "source": [
        "readme_message = f\"\"\"\n",
        "Thanks to @s3nh for the great quantization notebook code.\n",
        "---\n",
        "license: openrail\n",
        "pipeline_tag: text-generation\n",
        "library_name: transformers\n",
        "language:\n",
        "- en\n",
        "---\n",
        "\n",
        "\n",
        "## Original model card\n",
        "\n",
        "Buy @s3nh a coffee if you like this project ;)\n",
        "<a href=\"https://www.buymeacoffee.com/s3nh\"><img src=\"https://www.buymeacoffee.com/assets/img/guidelines/download-assets-sm-1.svg\" alt=\"\"></a>\n",
        "\n",
        "#### Description\n",
        "\n",
        "GGUF Format model files for [This project](https://huggingface.co/{MODEL_ID}).\n",
        "\n",
        "### GGUF Specs\n",
        "\n",
        "GGUF is a format based on the existing GGJT, but makes a few changes to the format to make it more extensible and easier to use. The following features are desired:\n",
        "\n",
        "Single-file deployment: they can be easily distributed and loaded, and do not require any external files for additional information.\n",
        "Extensible: new features can be added to GGML-based executors/new information can be added to GGUF models without breaking compatibility with existing models.\n",
        "mmap compatibility: models can be loaded using mmap for fast loading and saving.\n",
        "Easy to use: models can be easily loaded and saved using a small amount of code, with no need for external libraries, regardless of the language used.\n",
        "Full information: all information needed to load a model is contained in the model file, and no additional information needs to be provided by the user.\n",
        "The key difference between GGJT and GGUF is the use of a key-value structure for the hyperparameters (now referred to as metadata), rather than a list of untyped values.\n",
        "This allows for new metadata to be added without breaking compatibility with existing models, and to annotate the model with additional information that may be useful for\n",
        "inference or for identifying the model.\n",
        "\n",
        "# Original model card\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyweRSUGB7XW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "import pathlib\n",
        "from google.colab import userdata\n",
        "# Obtains the HuggingFace token from the colab secrets tab\n",
        "HF_TOKEN: str = userdata.get(\"HF_TOKEN\")\n",
        "REPOSITORY_NAME: str = f\"{MODEL_NAME}-GGUF\"\n",
        "\n",
        "api = HfApi()\n",
        "with open(f'{QUANT_PATH}/README.md', 'w') as outfile:\n",
        "    outfile.writelines(readme_message)\n",
        "gguf_files = list(pathlib.Path(f'/content/{QUANT_PATH}').rglob('*.gguf'))\n",
        "if len(gguf_files) > 0:\n",
        "\n",
        "    try:\n",
        "        create_repo(REPOSITORY_NAME, token=HF_TOKEN)\n",
        "\n",
        "        TYPE = 'model'\n",
        "        api.upload_folder(\n",
        "            folder_path = f'/content/{QUANT_PATH}',\n",
        "            repo_id = f'{USER}/{REPOSITORY_NAME}',\n",
        "            repo_type=TYPE,\n",
        "            path_in_repo = \"./\",\n",
        "            token= HF_TOKEN\n",
        "        )\n",
        "        !rm -rf {QUANT_PATH}/\n",
        "        !rm -rf {ORIG_PATH}/\n",
        "        !rm -rf {MODEL_NAME}/\n",
        "    except:\n",
        "        TYPE = 'model'\n",
        "        api.upload_folder(\n",
        "            folder_path = f'/content/{QUANT_PATH}',\n",
        "            repo_id = f'{USER}/{REPOSITORY_NAME}',\n",
        "            repo_type=TYPE,\n",
        "            path_in_repo = \"./\",\n",
        "            token= HF_TOKEN\n",
        "        )\n",
        "        !rm -rf {QUANT_PATH}/\n",
        "        !rm -rf {ORIG_PATH}/\n",
        "        !rm -rf {MODEL_NAME}/\n",
        "else:\n",
        "    print(\"Something went wrong\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkCcLKmD56XzY3SMPmZ4P5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}