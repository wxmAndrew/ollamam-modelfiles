{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/20_LLM_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYJoyfllAVBS"
      },
      "outputs": [],
      "source": [
        "!pip install rouge\n",
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "# install additional dependencies needed for training\n",
        "!pip install rouge-score tensorboard py7zr\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0sbCRLMBFWV",
        "outputId": "ddab1f1d-7e08-4fc1-abeb-a635fb05abaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "4nh8ZX_9wHpr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')\n"
      ],
      "metadata": {
        "id": "5eJOTa2RwIT2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gKae99qA93N",
        "outputId": "596a81fe-c92f-49ef-fff7-bc75455adf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def print_prime(n):\n",
            "   \"\"\"\n",
            "   Print all primes between 1 and n\n",
            "   \"\"\"\n",
            "   primes = []\n",
            "   for num in range(2, n+1):\n",
            "       is_prime = True\n",
            "       for i in range(2, int(num**0.5)+1):\n",
            "           if num % i == 0:\n",
            "               is_prime = False\n",
            "               break\n",
            "       if is_prime:\n",
            "           primes.append(num)\n",
            "   print(primes)\n",
            "\n",
            "print_prime(20)\n",
            "```\n",
            "\n",
            "## Exercises\n",
            "\n",
            "1. Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
            "\n",
            "```python\n",
            "def sum_even(numbers):\n",
            "   \"\"\"\n",
            "   Returns the sum of all even numbers in the list\n",
            "   \"\"\"\n",
            "   return sum(num for num in numbers if\n"
          ]
        }
      ],
      "source": [
        "torch.set_default_device('cuda')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "inputs = tokenizer('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Agq-4D9twJjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oKNWfV0rBBcU"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class LowRankConfig:\n",
        "    rank:int\n",
        "    target_modules: list[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frxqJod8Ba7H"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzAD12fkBizH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tm9bmIcPBz23"
      },
      "outputs": [],
      "source": [
        "#low rank decomposition of SelfAttention Key, Query and Value Matrices\n",
        "config = LowRankConfig(\n",
        "    rank= 384,\n",
        "    target_modules=[\"Wqkv\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n09Mv2V6B2ra"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from torch.nn import functional as F\n",
        "class LowRankLayer(nn.Module):\n",
        "    \"\"\"given a linear layer find low rank decomposition\"\"\"\n",
        "    def __init__(self, rank, full_rank_layer):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "\n",
        "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.double())\n",
        "        S_diag = torch.diag(S)\n",
        "        self.U = U[:, :self.rank].half()\n",
        "        self.S = S_diag[:self.rank, :self.rank].half()\n",
        "        self.Vh = Vh[:self.rank, :].half()\n",
        "\n",
        "    def forward(self, x):\n",
        "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
        "        output = F.linear(x, aprox_weight_matrix)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hXECniy_B4ZP"
      },
      "outputs": [],
      "source": [
        "#find the module that ends target suffix\n",
        "def get_submodules(model, key):\n",
        "    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n",
        "    target_name = key.split(\".\")[-1]\n",
        "    target = model.get_submodule(key)\n",
        "    return parent, target, target_name\n",
        "\n",
        "# this function replaces a target layer with low rank layer\n",
        "def recursive_setattr(obj, attr, value):\n",
        "    attr = attr.split('.', 1)\n",
        "    if len(attr) == 1:\n",
        "        setattr(obj, attr[0], value)\n",
        "    else:\n",
        "        recursive_setattr(getattr(obj, attr[0]), attr[1], value)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU4qlwNnkVBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FnYiv17sCAqk"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model_lr = copy.deepcopy(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LeJxtZ82CE-x"
      },
      "outputs": [],
      "source": [
        "for key, module in model.named_modules():\n",
        "    target_module_found = any(key.endswith(\".\" + target_key) for target_key in config.target_modules)\n",
        "    if target_module_found:\n",
        "        low_rank_layer = LowRankLayer(config.rank, module)\n",
        "        #replace target layer with low rank layer\n",
        "        recursive_setattr(model_lr, key, low_rank_layer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "hX32OjtOWxbh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CJGGyGNvCNdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b747ecc-375e-4f80-f7ee-b3919d6f3926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1418270720 || all params: 1418270720 || trainable%: 100.0\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fLjIyz-WuR8",
        "outputId": "b91819bd-fea2-4f06-dc7c-57aa729781d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1116133376 || all params: 1116133376 || trainable%: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1-1116133376/1418270720"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp5S4gBNyJPf",
        "outputId": "942a255c-6806-4a68-b907-b93d89b1d8a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2130322086886205"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"model\", from_pt=True)\n"
      ],
      "metadata": {
        "id": "Cs1r1yqjyUL1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr.save_pretrained(\"model_lr\", from_pt=True)\n"
      ],
      "metadata": {
        "id": "oE5UFsx2W3a6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh model/pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRdTj2GXBXK",
        "outputId": "3b05f25a-67b1-43c6-aa8a-388bae85d7a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 2.7G Sep 15 17:00 model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh model_lr/pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPsvu6eeXIq_",
        "outputId": "3ed6d9f6-ec65-4eed-9f89-375714cbc967"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 2.1G Sep 15 17:01 model_lr/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1-2.1/2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qVTIg2rycKd",
        "outputId": "49a09e7c-0bee-49c4-e0ff-566ff7258e7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2222222222222222"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "inputs = tokenizer('''```python\n",
        "def add(x,y):\n",
        "   ''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model_lr.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUoMIJ7lYGOo",
        "outputId": "18fde25e-391f-47f1-b5a3-ca9f1f56cccd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def add(x,y):\n",
            "   return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y):\n",
            "    return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y)\n",
            "    return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y)\n",
            "    return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y)\n",
            "    return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y)\n",
            "    return x + y\n",
            "\n",
            "# Test\n",
            "print(add(1,2)\n",
            "\n",
            "# Solution\n",
            "def add(x,y)\n",
            "    return\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('''```python\n",
        "def compare(a, b):\n",
        "    Compares two strings\n",
        "   ''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model_lr.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcKxU1k5YtqN",
        "outputId": "12f44ba2-95f4-4dae-a6a2-9e5d9b33d385"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def compare(a, b):\n",
            "    Compares two strings \n",
            "   \n",
            "    def compare(a, b):\n",
            "        if a == b:\n",
            "            return True\n",
            "        else:\n",
            "        return False\n",
            "\n",
            "# Test\n",
            "print(compare('hello', 'hello')\n",
            "\n",
            "# Output: True\n",
            "\n",
            "# Exercise 2\n",
            "def is_palindrome(s):\n",
            "    if s.count('a') == 0:\n",
            "        return True\n",
            "    else:\n",
            "        return False\n",
            "\n",
            "# Test\n",
            "print(is_palindrome('hello')\n",
            "\n",
            "# Output: True\n",
            "\n",
            "# Exercise 3\n",
            "def is_palindrome(s):\n",
            "    if s.count('a') == 0:\n",
            "        return True\n",
            "    else:\n",
            "        return False\n",
            "\n",
            "# Test\n",
            "print(is_palindrome('hello')\n",
            "\n",
            "# Exercise 3\n",
            "def is_palindrome(s):\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('''```python\n",
        "def sum(array):\n",
        "    For loop to compute sum of numbers in an array\n",
        "   ''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model_lr.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "800mHfPhytDz",
        "outputId": "571cde9a-d1ce-4d87-9c39-144f1f500dc4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def sum(array):\n",
            "    For loop to compute sum of numbers in an array\n",
            "   \n",
            "    def sum(array):\n",
            "        sum = 0\n",
            "    for i in array:\n",
            "        sum += i\n",
            "    return sum\n",
            "\n",
            "# Test\n",
            "print(sum([1, 2, 3, 4, 5] )\n",
            "\n",
            "# Solution\n",
            "def sum(array):\n",
            "    sum = 0\n",
            "    for i in array:\n",
            "        sum += i\n",
            "    return sum\n",
            "\n",
            "# Test\n",
            "print(sum([1, 2, 3, 4, 5] )\n",
            "\n",
            "# Solution\n",
            "def sum(array):\n",
            "    sum = 0\n",
            "    for i in array:\n",
            "        sum += i\n",
            "    return sum\n",
            "\n",
            "# Test\n",
            "print(sum([1, 2, 3, 4, 5] )\n",
            "\n",
            "# Solution\n",
            "def sum(array):\n",
            "    sum = 0\n",
            "    for i in array:\n",
            "        sum += i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sk8MZ9jPztzj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZBZEO4zFBTAdBZCL1jeVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}